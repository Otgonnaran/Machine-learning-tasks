import numpy as np
import matplotlib.pyplot as plt

class ScratchLogisticRegression():
    """
    Parameters
    ----------
    num_iter : int
    lr : float
    no_bias : bool
    verbose : bool

    Attributes
    ----------
    self.coef_ : ndarray, shape (n_features,)
    self.loss : ndarray, shape (self.iter,)
    self.val_loss : ndarray, shape (self.iter,)
    """

    def __init__(self, num_iter, lr, no_bias, verbose):
        self.iter = num_iter
        self.lr = lr
        self.no_bias = no_bias
        self.verbose = verbose
        self.loss = np.zeros(self.iter)
        self.val_loss = np.zeros(self.iter)

    def fit(self, X, y, X_val=None, y_val=None):
        """
        Parameters
        ----------
        X : ndarray, shape (n_samples, n_features)
        y : ndarray, shape (n_samples, )

        X_val : ndarray, shape (n_samples, n_features)
        y_val : ndarray, shape (n_samples, )
        """
        if not self.no_bias:
            X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)
            if X_val is not None:
                X_val = np.concatenate([X_val, np.ones((X_val.shape[0], 1))], axis=1)

        self.coef_ = np.random.random((X.shape[1],))

        for epoch in range(self.iter):
            y_pred = self._logit_hypothesis(X)
            self.loss[epoch] = self._calculate_loss(y, y_pred)
            self._gradient_descent(X, y, y_pred)

            if X_val is not None:
                y_pred_val = self._logit_hypothesis(X_val)
                self.val_loss[epoch] = self._calculate_loss(y_val, y_pred_val)

            if self.verbose:
                if X_val is not None:
                    print(f"Epoch-{epoch}: train loss={self.loss[epoch]}, "
                          f"val loss={self.val_loss[epoch]}")
                else:
                    print(f"Epoch-{epoch}: train loss={self.loss[epoch]}")

        return self.coef_, self.loss, self.val_loss

    def predict(self, X):
        """
        Parameters
        ----------
        X : ndarray, shape (n_samples, n_features)
        Returns
        -------
        ndarray, shape (n_samples, 1)
        """
        prob = self._logit_hypothesis(X)
        y_pred = np.where(prob >= 0.5, 1, 0)

        return y_pred

    def predict_proba(self, X):
        """
        Parameters
        ----------
        X : ndarray, shape (n_samples, n_features)
        Returns
        -------
        ndarray, shape (n_samples, 1)
        """
        prob = self._logit_hypothesis(X)

        return prob

    def _logit_hypothesis(self, X):
        """
        Parameters
        ----------
        X : ndarray, shape (n_samples, n_features)
        Returns
        -------
        ndarray, shape (n_samples, 1)
        """
        z = np.dot(X, self.coef_)
        y_pred = self.sigmoid(z)

        return y_pred

    def sigmoid(self, z):
        prob = 1 / (1 + np.exp(-z))
        return prob

    def _gradient_descent(self, X, y, y_pred):
        """
        Update parameters using steepest descent

        Parameters
        ----------
        X : array-like
            Input features
        y : array-like
            Correct answer labels
        y_pred : array-like
            Predicted labels
        """
        error = y_pred - y
        gradient = np.mean(X.T * error, axis=1)
        self.coef_ -= self.lr * gradient

    def _calculate_loss(self, y, y_pred):
        """
        Calculate the loss function with regularization term

        Parameters
        ----------
        y : ndarray, shape (n_samples,)
            Correct answer labels
        y_pred : ndarray, shape (n_samples,)
            Predicted labels

        Returns
        -------
        float
        """
        regularization_term = (self.lr / (2 * len(self.coef_))) * np.sum(self.coef_[1:]**2)
        loss = np.mean((-y * np.log(y_pred) - (1 - y) * np.log(1 - y_pred))) + regularization_term

        return loss

    def visualize_decision_area(self, X, y):
        """
        Visualize the decision area using contour plots.

        Parameters
        ----------
        X : ndarray, shape (n_samples, n_features)
            Input features
        y : ndarray, shape (n_samples,)
            Correct answer labels
        """
        if X.shape[1] != 2:
            print("Visualization is only supported for 2D features.")
            return

        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1

        xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),
                             np.arange(y_min, y_max, 0.01))

        Z = self.predict(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)

        plt.contourf(xx, yy, Z, alpha=0.3)
        plt.scatter(X[:, 0], X[:, 1], c=y, marker='o', edgecolors='k')
        plt.xlabel('Feature 1')
        plt.ylabel('Feature 2')
        plt.title('Decision Area Visualization')
        plt.show()

# Test the implementation
X = np.random.random((100, 2))
y = np.random.randint(low=0, high=2, size=(100,))

X_val = np.random.random((100, 2))
y_val = np.random.randint(low=0, high=2, size=(100,))

n_iter = 100
lr = 0.001
bias = True
verbose = True

linear_model = ScratchLogisticRegression(num_iter=n_iter,
                                         lr=lr,
                                         no_bias=bias,
                                         verbose=verbose)

model, train_loss, val_loss = linear_model.fit(X, y, X_val, y_val)

plt.plot(train_loss, label='train_loss')
plt.plot(val_loss, label='val_loss')
plt.legend()
plt.show()

# Visualize the decision area
linear_model.visualize_decision_area(X, y)
